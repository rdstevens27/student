{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "listshift_newbegin.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOdDaS4ByHJb/tsrx3sgAlJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rdstevens27/student/blob/master/listshift_newbegin.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtvSxW8zfSEm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9d078bb8-1f98-4c43-8b99-c2943627071e"
      },
      "source": [
        "# Pushing 1st range data from 1st Nov 2020 to 31st jan 2020 final code   1st version\r\n",
        "\r\n",
        "import requests\r\n",
        "import json\r\n",
        "from datetime import datetime\r\n",
        "from datetime import timedelta\r\n",
        "import os\r\n",
        "import hashlib\r\n",
        "import datetime as dt\r\n",
        "import pandas as pd\r\n",
        "from copy import deepcopy\r\n",
        "from google.cloud import bigquery\r\n",
        "from google.oauth2 import service_account\r\n",
        "from google.cloud.exceptions import NotFound\r\n",
        "from pandas import DataFrame\r\n",
        "\r\n",
        "project_id = 'hnmc-data-science'\r\n",
        "dataset_id = 'wheniwork'\r\n",
        "table_id = 'listShifts'\r\n",
        "\r\n",
        "# TODO(developer): Set key_path to the path to the service account key\r\n",
        "#                  file.\r\n",
        "\r\n",
        "key_path = \"/hnmc-data-science-1e5996a1f37c.json\"\r\n",
        "\r\n",
        "credentials = service_account.Credentials.from_service_account_file(\r\n",
        "    key_path, scopes=[\"https://www.googleapis.com/auth/cloud-platform\"],\r\n",
        ")\r\n",
        "client = bigquery.Client(credentials=credentials, project=project_id)\r\n",
        "dataset = client.dataset(dataset_id)\r\n",
        "table_ref = dataset.table(table_id)\r\n",
        "\r\n",
        "time_schema = [\r\n",
        "    bigquery.SchemaField(\"current_time\", \"TIMESTAMP\", mode=\"REQUIRED\")]\r\n",
        "\r\n",
        "listShift_schema = [\r\n",
        "       bigquery.SchemaField(\"id\", \"INT64\", mode=\"REQUIRED\"),\r\n",
        "       bigquery.SchemaField(\"account_id\", \"INT64\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"user_id\", \"INT64\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"location_id\", \"INT64\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"position_id\", \"INT64\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"site_id\", \"INT64\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"start_time\", \"TIMESTAMP\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"end_time\", \"TIMESTAMP\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"break_time\", \"FLOAT64\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"color\", \"STRING\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"notes\", \"STRING\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"alerted\", \"BOOL\", mode=\"NULLABLE\"),\r\n",
        "      # bigquery.SchemaField(\"linked_users\", \"INT64\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"shiftchain_key\", \"STRING\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"published\", \"BOOL\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"published_date\", \"TIMESTAMP\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"notified_at\", \"STRING\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"instances\", \"INT64\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"created_at\", \"TIMESTAMP\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"updated_at\", \"TIMESTAMP\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"acknowledged\", \"INT64\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"acknowledged_at\", \"TIMESTAMP\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"creator_id\", \"INT64\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"is_open\", \"BOOL\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"actionable\", \"BOOL\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"block_id\", \"INT64\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"requires_openshift_approval\", \"BOOL\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"openshift_approval_request_id\", \"INT64\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"is_shared\", \"INT64\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"UNIQUE_KEY\", \"STRING\", mode=\"REQUIRED\"),\r\n",
        "       bigquery.SchemaField(\"VERSION_DATE\", \"TIMESTAMP\", mode=\"NULLABLE\")\r\n",
        "       ]\r\n",
        "\r\n",
        "def main():\r\n",
        "\r\n",
        "   df = get_all_data()\r\n",
        "   load_to_bigquery(df)\r\n",
        "\r\n",
        "  #  last_query_time_df = fetch_data()\r\n",
        "  #  get_all_data(last_query_time_df)\r\n",
        "\r\n",
        "# def fetch_data():\r\n",
        "\r\n",
        "#     sql_source = \"\"\" SELECT * FROM `hnmc-data-science.wheniwork.currentShiftTime` \"\"\"             \r\n",
        "\r\n",
        "#     df_source = client.query(sql_source).to_dataframe()\r\n",
        "\r\n",
        "#     return df_source\r\n",
        "\r\n",
        "def get_all_data():\r\n",
        "\r\n",
        "    r = requests.post('https://api.login.wheniwork.com/login', json={\"email\":\"rdoshi@holyname.org\",\"password\":\"wheniwork123\"})\r\n",
        "    r.status_code\r\n",
        "    data= r.json()\r\n",
        "\r\n",
        "    token=data[\"token\"]\r\n",
        "    #print(token)\r\n",
        "    endpoint = \"https://api.wheniwork.com/2/shifts\"\r\n",
        "\r\n",
        "    headers = {\"Authorization\": token}\r\n",
        "\r\n",
        "    all_data = requests.get(endpoint, params={'start': '01 Nov 2020', 'end': '31 Jan 2021', 'include_allopen': True}, headers=headers).json()\r\n",
        "    print(all_data)\r\n",
        "\r\n",
        "    id_ofall_data = []\r\n",
        "    for i in range(0, len(all_data['shifts'])):\r\n",
        "\r\n",
        "      id_ofall_data.append(all_data['shifts'][i]['id'])\r\n",
        "\r\n",
        "    print(id_ofall_data)\r\n",
        "\r\n",
        "    new_list = []\r\n",
        "    for i in range(0, len(all_data['shifts'])):\r\n",
        "\r\n",
        "      if all_data['shifts'][i]['id'] in id_ofall_data: \r\n",
        "        \r\n",
        "        #print(all_data['shifts'][i])\r\n",
        "        new_list.append(all_data['shifts'][i])\r\n",
        "\r\n",
        "    # convert list of dictionary to dataframe\r\n",
        "\r\n",
        "    #print(new_list)\r\n",
        "    df = pd.DataFrame(new_list)\r\n",
        "\r\n",
        "    df.to_csv(\"before_time_conv.csv\", index = False)\r\n",
        "    df['start_time'] = df['start_time'].astype(str).str[5:-6]\r\n",
        "    df['end_time'] = df['end_time'].astype(str).str[5:-6]\r\n",
        "    df['published_date'] = df['published_date'].astype(str).str[5:-6]\r\n",
        "    df['notified_at'] = df['notified_at'].astype(str).str[5:-6]\r\n",
        "    df['created_at'] = df['created_at'].astype(str).str[5:-6]\r\n",
        "    df['updated_at'] = df['updated_at'].astype(str).str[5:-6]\r\n",
        "    df['acknowledged_at'] = df['acknowledged_at'].astype(str).str[5:-6]\r\n",
        "    df.to_csv(\"after_time_conv.csv\", index = False)\r\n",
        "    # print(df)\r\n",
        "    df['start_time'] =  pd.to_datetime(df['start_time'], format='%d %b %Y %H:%M:%S', errors='ignore')\r\n",
        "    df['end_time'] =  pd.to_datetime(df['end_time'], format='%d %b %Y %H:%M:%S', errors='ignore')\r\n",
        "    df['published_date'] =  pd.to_datetime(df['published_date'], format='%d %b %Y %H:%M:%S', errors='ignore')\r\n",
        "    df['notified_at'] =  pd.to_datetime(df['notified_at'], format='%d %b %Y %H:%M:%S', errors='ignore')\r\n",
        "    df['created_at'] =  pd.to_datetime(df['created_at'], format='%d %b %Y %H:%M:%S', errors='ignore')\r\n",
        "    df['updated_at'] =  pd.to_datetime(df['updated_at'], format='%d %b %Y %H:%M:%S', errors='ignore')\r\n",
        "    df['acknowledged_at'] =  pd.to_datetime(df['acknowledged_at'], format='%d %b %Y %H:%M:%S', errors='ignore')\r\n",
        "   \r\n",
        "    print(df['updated_at'].dtypes)\r\n",
        "    #generating Hash key\r\n",
        "    gene_hash = {\r\n",
        "        'listShifts': ['id']\r\n",
        "    }\r\n",
        "\r\n",
        "    key_combination = gene_hash.get('listShifts')\r\n",
        "    \r\n",
        "    df['UNIQUE_KEY'] = df[key_combination].apply(lambda row: '_'.join(row.values.astype(\r\n",
        "        str)), axis=1).astype(str).str.encode('utf-8').apply(lambda x: (hashlib.sha512(x).hexdigest().upper()))\r\n",
        "\r\n",
        "    df['VERSION_DATE'] = datetime.now()\r\n",
        "    print(df.info())\r\n",
        "    df.to_csv(\"linked_user.csv\", index = False)\r\n",
        "    df.drop('linked_users', axis=1, inplace=True)\r\n",
        "    print(df.info())\r\n",
        "    df.to_csv(\"linked_user_rm.csv\", index = False)\r\n",
        "    # # check for updated data from API call\r\n",
        "    # print(df['updated_at'])\r\n",
        "    # print(last_query_time_df['current_time'][0])\r\n",
        "    # last_query_time_df['current_time'] = last_query_time_df['current_time'].astype(str).str[:-6]\r\n",
        "    # last_query_time_df['current_time'] =  pd.to_datetime(last_query_time_df['current_time'], format='%d %b %Y %H:%M:%S', errors='ignore')\r\n",
        "    # print(last_query_time_df['current_time'][0])\r\n",
        "    # new_data_df = df[df['updated_at'] >= (last_query_time_df['current_time'][0])]\r\n",
        "    # load_to_bigquery(new_data_df)\r\n",
        "\r\n",
        "    # Assign current time to our new currentShiftTime table\r\n",
        "\r\n",
        "    current_shift_time = datetime.now() - timedelta(hours=5)\r\n",
        "    current_time = {'current_time': current_shift_time.strftime(\"%Y-%m-%d %H:%M:%S\")}\r\n",
        "    \r\n",
        "    current_time_df = pd.DataFrame(current_time, columns = ['current_time'], index=[0])\r\n",
        "\r\n",
        "    current_time_df['current_time'] =  pd.to_datetime(current_time_df['current_time'], format='%Y-%m-%d %H:%M:%S')\r\n",
        "\r\n",
        "    load_to_bigquery_current_time(current_time_df, time_schema)\r\n",
        "\r\n",
        "    # df.to_csv(\"datetime.csv\", index = False)\r\n",
        "\r\n",
        "    return df\r\n",
        "\r\n",
        "def load_to_bigquery(df):\r\n",
        "\r\n",
        "    df.to_csv(\"test1.csv\", index = False)\r\n",
        "    print(\"Inside Bigquery \")\r\n",
        "    table_id = \"hnmc-data-science.wheniwork.listShifts\"\r\n",
        "\r\n",
        "    # Specifying all table schema\r\n",
        "    # with open('/listShift.json') as json_schema: \r\n",
        "    #     file_schema = json.load(json_schema)\r\n",
        "        \r\n",
        "    print(\"Inside schema\")\r\n",
        "    #print(file_schema['listShifts'])\r\n",
        "    job_config = bigquery.LoadJobConfig(\r\n",
        "        \r\n",
        "      schema = listShift_schema,  \r\n",
        "      write_disposition=\"WRITE_APPEND\"\r\n",
        "    # Optionally, set the write disposition. BigQuery appends loaded rows\r\n",
        "    # to an existing table by default, but with WRITE_TRUNCATE write\r\n",
        "    # disposition it replaces the table with the loaded data.\r\n",
        "    )\r\n",
        "\r\n",
        "    job = client.load_table_from_dataframe(\r\n",
        "        df, table_id, job_config=job_config\r\n",
        "    )  # Make an API request.\r\n",
        "    job.result()  # Wait for the job to complete.\r\n",
        "\r\n",
        "    table = client.get_table(table_id)  # Make an API request.\r\n",
        "    print(\r\n",
        "        \"Loaded {} rows and {} columns to {}\".format(\r\n",
        "            table.num_rows, len(table.schema), table_id\r\n",
        "        )\r\n",
        "    )\r\n",
        "\r\n",
        "def load_to_bigquery_current_time(df, time_schema):\r\n",
        "\r\n",
        "    print(\"Inside Bigquery \")\r\n",
        "    project_id = 'hnmc-data-science'\r\n",
        "    dataset_id = 'wheniwork'\r\n",
        "\r\n",
        "    table_id = f\"{project_id}.{dataset_id}.currentShiftTime\"\r\n",
        "\r\n",
        "    # Specifying all table schema\r\n",
        "    # with open('/listShift.json') as json_schema: \r\n",
        "    #     file_schema = json.load(json_schema)\r\n",
        "    # print(type(file_schema[filename]))  \r\n",
        "    print(\"Inside schema\")\r\n",
        "   \r\n",
        "    #print(file_schema['listShifts'])\r\n",
        "    job_config = bigquery.LoadJobConfig(\r\n",
        "          \r\n",
        "      write_disposition=\"WRITE_TRUNCATE\", schema = time_schema\r\n",
        "      #, encoding = 'ISO-8859-1'\r\n",
        "    # Optionally, set the write disposition. BigQuery appends loaded rows\r\n",
        "    # to an existing table by default, but with WRITE_TRUNCATE write\r\n",
        "    # disposition it replaces the table with the loaded data.\r\n",
        "    )\r\n",
        "\r\n",
        "    job = client.load_table_from_dataframe(\r\n",
        "        df, table_id, job_config=job_config\r\n",
        "    )  # Make an API request.\r\n",
        "    job.result()  # Wait for the job to complete.\r\n",
        "\r\n",
        "    table = client.get_table(table_id)  # Make an API request.\r\n",
        "    print(\r\n",
        "        \"Loaded {} rows and {} columns to {}\".format(\r\n",
        "            table.num_rows, len(table.schema), table_id\r\n",
        "        )\r\n",
        "    )\r\n",
        "  \r\n",
        "main()"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "datetime64[ns]\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 4634 entries, 0 to 4633\n",
            "Data columns (total 31 columns):\n",
            " #   Column                         Non-Null Count  Dtype         \n",
            "---  ------                         --------------  -----         \n",
            " 0   id                             4634 non-null   int64         \n",
            " 1   account_id                     4634 non-null   int64         \n",
            " 2   user_id                        4634 non-null   int64         \n",
            " 3   location_id                    4634 non-null   int64         \n",
            " 4   position_id                    4634 non-null   int64         \n",
            " 5   site_id                        4634 non-null   int64         \n",
            " 6   start_time                     4634 non-null   datetime64[ns]\n",
            " 7   end_time                       4634 non-null   datetime64[ns]\n",
            " 8   break_time                     4634 non-null   float64       \n",
            " 9   color                          4634 non-null   object        \n",
            " 10  notes                          4634 non-null   object        \n",
            " 11  alerted                        4634 non-null   bool          \n",
            " 12  linked_users                   0 non-null      object        \n",
            " 13  shiftchain_key                 4634 non-null   object        \n",
            " 14  published                      4634 non-null   bool          \n",
            " 15  published_date                 4634 non-null   datetime64[ns]\n",
            " 16  notified_at                    4634 non-null   object        \n",
            " 17  instances                      4634 non-null   int64         \n",
            " 18  created_at                     4634 non-null   datetime64[ns]\n",
            " 19  updated_at                     4634 non-null   datetime64[ns]\n",
            " 20  acknowledged                   4634 non-null   int64         \n",
            " 21  acknowledged_at                4634 non-null   object        \n",
            " 22  creator_id                     4634 non-null   int64         \n",
            " 23  is_open                        4634 non-null   bool          \n",
            " 24  actionable                     4634 non-null   bool          \n",
            " 25  block_id                       4634 non-null   int64         \n",
            " 26  requires_openshift_approval    4634 non-null   bool          \n",
            " 27  openshift_approval_request_id  4634 non-null   int64         \n",
            " 28  is_shared                      4634 non-null   int64         \n",
            " 29  UNIQUE_KEY                     4634 non-null   object        \n",
            " 30  VERSION_DATE                   4634 non-null   datetime64[ns]\n",
            "dtypes: bool(5), datetime64[ns](6), float64(1), int64(12), object(7)\n",
            "memory usage: 964.0+ KB\n",
            "None\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 4634 entries, 0 to 4633\n",
            "Data columns (total 30 columns):\n",
            " #   Column                         Non-Null Count  Dtype         \n",
            "---  ------                         --------------  -----         \n",
            " 0   id                             4634 non-null   int64         \n",
            " 1   account_id                     4634 non-null   int64         \n",
            " 2   user_id                        4634 non-null   int64         \n",
            " 3   location_id                    4634 non-null   int64         \n",
            " 4   position_id                    4634 non-null   int64         \n",
            " 5   site_id                        4634 non-null   int64         \n",
            " 6   start_time                     4634 non-null   datetime64[ns]\n",
            " 7   end_time                       4634 non-null   datetime64[ns]\n",
            " 8   break_time                     4634 non-null   float64       \n",
            " 9   color                          4634 non-null   object        \n",
            " 10  notes                          4634 non-null   object        \n",
            " 11  alerted                        4634 non-null   bool          \n",
            " 12  shiftchain_key                 4634 non-null   object        \n",
            " 13  published                      4634 non-null   bool          \n",
            " 14  published_date                 4634 non-null   datetime64[ns]\n",
            " 15  notified_at                    4634 non-null   object        \n",
            " 16  instances                      4634 non-null   int64         \n",
            " 17  created_at                     4634 non-null   datetime64[ns]\n",
            " 18  updated_at                     4634 non-null   datetime64[ns]\n",
            " 19  acknowledged                   4634 non-null   int64         \n",
            " 20  acknowledged_at                4634 non-null   object        \n",
            " 21  creator_id                     4634 non-null   int64         \n",
            " 22  is_open                        4634 non-null   bool          \n",
            " 23  actionable                     4634 non-null   bool          \n",
            " 24  block_id                       4634 non-null   int64         \n",
            " 25  requires_openshift_approval    4634 non-null   bool          \n",
            " 26  openshift_approval_request_id  4634 non-null   int64         \n",
            " 27  is_shared                      4634 non-null   int64         \n",
            " 28  UNIQUE_KEY                     4634 non-null   object        \n",
            " 29  VERSION_DATE                   4634 non-null   datetime64[ns]\n",
            "dtypes: bool(5), datetime64[ns](6), float64(1), int64(12), object(6)\n",
            "memory usage: 927.8+ KB\n",
            "None\n",
            "Inside Bigquery \n",
            "Inside schema\n",
            "Loaded 1 rows and 1 columns to hnmc-data-science.wheniwork.currentShiftTime\n",
            "Inside Bigquery \n",
            "Inside schema\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ArrowTypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mArrowTypeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-49-3fe9f1dfbae5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    247\u001b[0m     )\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-49-3fe9f1dfbae5>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m    \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_all_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m    \u001b[0mload_to_bigquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m   \u001b[0;31m#  last_query_time_df = fetch_data()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-49-3fe9f1dfbae5>\u001b[0m in \u001b[0;36mload_to_bigquery\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m     job = client.load_table_from_dataframe(\n\u001b[0;32m--> 202\u001b[0;31m         \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjob_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m     )  # Make an API request.\n\u001b[1;32m    204\u001b[0m     \u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Wait for the job to complete.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/cloud/bigquery/client.py\u001b[0m in \u001b[0;36mload_table_from_dataframe\u001b[0;34m(self, dataframe, destination, num_retries, job_id, job_id_prefix, location, project, job_config, parquet_compression)\u001b[0m\n\u001b[1;32m   1608\u001b[0m                     \u001b[0mjob_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1609\u001b[0m                     \u001b[0mtmppath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1610\u001b[0;31m                     \u001b[0mparquet_compression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparquet_compression\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1611\u001b[0m                 )\n\u001b[1;32m   1612\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/cloud/bigquery/_pandas_helpers.py\u001b[0m in \u001b[0;36mdataframe_to_parquet\u001b[0;34m(dataframe, bq_schema, filepath, parquet_compression)\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pyarrow is required for BigQuery schema conversion.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m     \u001b[0marrow_table\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataframe_to_arrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbq_schema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m     \u001b[0mpyarrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrow_table\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparquet_compression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/cloud/bigquery/_pandas_helpers.py\u001b[0m in \u001b[0;36mdataframe_to_arrow\u001b[0;34m(dataframe, bq_schema)\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0marrow_names\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbq_field\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m         arrow_arrays.append(\n\u001b[0;32m--> 335\u001b[0;31m             \u001b[0mbq_to_arrow_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_column_or_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbq_field\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbq_field\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m         )\n\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/cloud/bigquery/_pandas_helpers.py\u001b[0m in \u001b[0;36mbq_to_arrow_array\u001b[0;34m(series, bq_field)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbq_field\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfield_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_STRUCT_TYPES\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpyarrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStructArray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marrow_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mpyarrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marrow_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyarrow/array.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.array\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyarrow/array.pxi\u001b[0m in \u001b[0;36mpyarrow.lib._ndarray_to_array\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyarrow/error.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mArrowTypeError\u001b[0m: an integer is required (got type str)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9mKRwTTdiU3Z",
        "outputId": "0b5610d9-fd25-45e4-e88e-f0dedc4cf24b"
      },
      "source": [
        "# Pushing 1st range data from 1st Nov 2020 to 31st jan 2020 final code  ---- second version of code while working with Updated_at >  current time\r\n",
        "\r\n",
        "import requests\r\n",
        "import json\r\n",
        "from datetime import datetime\r\n",
        "from datetime import timedelta\r\n",
        "import os\r\n",
        "import hashlib\r\n",
        "import datetime as dt\r\n",
        "import pandas as pd\r\n",
        "from copy import deepcopy\r\n",
        "from google.cloud import bigquery\r\n",
        "from google.oauth2 import service_account\r\n",
        "from google.cloud.exceptions import NotFound\r\n",
        "from pandas import DataFrame\r\n",
        "\r\n",
        "project_id = 'hnmc-data-science'\r\n",
        "dataset_id = 'wheniwork'\r\n",
        "table_id = 'listShifts'\r\n",
        "\r\n",
        "# TODO(developer): Set key_path to the path to the service account key\r\n",
        "#                  file.\r\n",
        "\r\n",
        "key_path = \"/hnmc-data-science-1e5996a1f37c.json\"\r\n",
        "\r\n",
        "credentials = service_account.Credentials.from_service_account_file(\r\n",
        "    key_path, scopes=[\"https://www.googleapis.com/auth/cloud-platform\"],\r\n",
        ")\r\n",
        "client = bigquery.Client(credentials=credentials, project=project_id)\r\n",
        "dataset = client.dataset(dataset_id)\r\n",
        "table_ref = dataset.table(table_id)\r\n",
        "\r\n",
        "time_schema = [\r\n",
        "    bigquery.SchemaField(\"current_time\", \"TIMESTAMP\", mode=\"REQUIRED\")]\r\n",
        "\r\n",
        "listShift_schema = [\r\n",
        "       bigquery.SchemaField(\"id\", \"INT64\", mode=\"REQUIRED\"),\r\n",
        "       bigquery.SchemaField(\"account_id\", \"INT64\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"user_id\", \"INT64\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"location_id\", \"INT64\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"position_id\", \"INT64\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"site_id\", \"INT64\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"start_time\", \"TIMESTAMP\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"end_time\", \"TIMESTAMP\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"break_time\", \"FLOAT64\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"color\", \"STRING\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"notes\", \"STRING\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"alerted\", \"BOOL\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"linked_users\", \"INT64\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"shiftchain_key\", \"STRING\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"published\", \"BOOL\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"published_date\", \"TIMESTAMP\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"notified_at\", \"TIMESTAMP\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"instances\", \"INT64\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"created_at\", \"TIMESTAMP\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"updated_at\", \"TIMESTAMP\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"acknowledged\", \"INT64\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"acknowledged_at\", \"TIMESTAMP\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"creator_id\", \"INT64\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"is_open\", \"BOOL\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"actionable\", \"BOOL\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"block_id\", \"INT64\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"requires_openshift_approval\", \"BOOL\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"openshift_approval_request_id\", \"INT64\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"is_shared\", \"INT64\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"UNIQUE_KEY\", \"STRING\", mode=\"REQUIRED\"),\r\n",
        "       bigquery.SchemaField(\"VERSION_DATE\", \"TIMESTAMP\", mode=\"NULLABLE\")\r\n",
        "       ]\r\n",
        "\r\n",
        "def main():\r\n",
        "\r\n",
        "  #  df = get_all_data()\r\n",
        "  #  load_to_bigquery(df)\r\n",
        "\r\n",
        "   last_query_time_df = fetch_data()\r\n",
        "   get_all_data(last_query_time_df)\r\n",
        "\r\n",
        "def fetch_data():\r\n",
        "\r\n",
        "    sql_source = \"\"\" SELECT * FROM `hnmc-data-science.wheniwork.currentShiftTime` \"\"\"             \r\n",
        "\r\n",
        "    df_source = client.query(sql_source).to_dataframe()\r\n",
        "    df_source['current_time'] = df_source['current_time'].astype(str).str[:-6]\r\n",
        "    df_source['current_time'] = pd.to_datetime(df_source['current_time'], format='%d %b %Y %H:%M:%S', errors='ignore')\r\n",
        "\r\n",
        "    return df_source\r\n",
        "\r\n",
        "def get_all_data(last_query_time_df):\r\n",
        "\r\n",
        "    r = requests.post('https://api.login.wheniwork.com/login', json={\"email\":\"rdoshi@holyname.org\",\"password\":\"wheniwork123\"})\r\n",
        "    r.status_code\r\n",
        "    data= r.json()\r\n",
        "\r\n",
        "    token=data[\"token\"]\r\n",
        "    #print(token)\r\n",
        "    endpoint = \"https://api.wheniwork.com/2/shifts\"\r\n",
        "\r\n",
        "    headers = {\"Authorization\": token}\r\n",
        "\r\n",
        "    all_data = requests.get(endpoint, params={'start': '01 Nov 2020', 'end': '31 Jan 2021', 'include_allopen': True}, headers=headers).json()\r\n",
        "    print(all_data)\r\n",
        "\r\n",
        "    id_ofall_data = []\r\n",
        "    for i in range(0, len(all_data['shifts'])):\r\n",
        "\r\n",
        "      id_ofall_data.append(all_data['shifts'][i]['id'])\r\n",
        "\r\n",
        "    print(id_ofall_data)\r\n",
        "\r\n",
        "    new_list = []\r\n",
        "    for i in range(0, len(all_data['shifts'])):\r\n",
        "\r\n",
        "      if all_data['shifts'][i]['id'] in id_ofall_data: \r\n",
        "        \r\n",
        "        #print(all_data['shifts'][i])\r\n",
        "        new_list.append(all_data['shifts'][i])\r\n",
        "\r\n",
        "    # convert list of dictionary to dataframe\r\n",
        "\r\n",
        "    #print(new_list)\r\n",
        "    df = pd.DataFrame(new_list)\r\n",
        "\r\n",
        "    df.to_csv(\"before_time_conv.csv\", index = False)\r\n",
        "    df['start_time'] = df['start_time'].astype(str).str[5:-6]\r\n",
        "    df['end_time'] = df['end_time'].astype(str).str[5:-6]\r\n",
        "    df['published_date'] = df['published_date'].astype(str).str[5:-6]\r\n",
        "    df['notified_at'] = df['notified_at'].astype(str).str[5:-6]\r\n",
        "    df['created_at'] = df['created_at'].astype(str).str[5:-6]\r\n",
        "    df['updated_at'] = df['updated_at'].astype(str).str[5:-6]\r\n",
        "    df['acknowledged_at'] = df['acknowledged_at'].astype(str).str[5:-6]\r\n",
        "    df.to_csv(\"after_time_conv.csv\", index = False)\r\n",
        "    # print(df)\r\n",
        "    df['start_time'] =  pd.to_datetime(df['start_time'], format='%d %b %Y %H:%M:%S', errors='ignore')\r\n",
        "    df['end_time'] =  pd.to_datetime(df['end_time'], format='%d %b %Y %H:%M:%S', errors='ignore')\r\n",
        "    df['published_date'] =  pd.to_datetime(df['published_date'], format='%d %b %Y %H:%M:%S', errors='ignore')\r\n",
        "    df['notified_at'] =  pd.to_datetime(df['notified_at'], format='%d %b %Y %H:%M:%S', errors='ignore')\r\n",
        "    df['created_at'] =  pd.to_datetime(df['created_at'], format='%d %b %Y %H:%M:%S', errors='ignore')\r\n",
        "    df['updated_at'] =  pd.to_datetime(df['updated_at'], format='%d %b %Y %H:%M:%S', errors='ignore')\r\n",
        "    df['acknowledged_at'] =  pd.to_datetime(df['acknowledged_at'], format='%d %b %Y %H:%M:%S', errors='ignore')\r\n",
        "   \r\n",
        "    print(df['updated_at'].dtypes)\r\n",
        "    #generating Hash key\r\n",
        "    gene_hash = {\r\n",
        "        'listShifts': ['id']\r\n",
        "    }\r\n",
        "\r\n",
        "    key_combination = gene_hash.get('listShifts')\r\n",
        "    \r\n",
        "    df['UNIQUE_KEY'] = df[key_combination].apply(lambda row: '_'.join(row.values.astype(\r\n",
        "        str)), axis=1).astype(str).str.encode('utf-8').apply(lambda x: (hashlib.sha512(x).hexdigest().upper()))\r\n",
        "\r\n",
        "    df['VERSION_DATE'] = datetime.now()\r\n",
        "\r\n",
        "    # check for updated data from API call\r\n",
        "\r\n",
        "    print(df['updated_at'])\r\n",
        "    print(last_query_time_df['current_time'][0])\r\n",
        "    # last_query_time_df['current_time'] = last_query_time_df['current_time'].astype(str).str[:-6]\r\n",
        "    # last_query_time_df['current_time'] =  pd.to_datetime(last_query_time_df['current_time'], format='%d %b %Y %H:%M:%S', errors='ignore')\r\n",
        "    print(last_query_time_df['current_time'][0])\r\n",
        "    new_data_df = df[df['updated_at'] >= (last_query_time_df['current_time'][0])]\r\n",
        "    print(new_data_df.info())\r\n",
        "    load_to_bigquery(new_data_df, listShift_schema)\r\n",
        "\r\n",
        "    # Assign current time to our new currentShiftTime table\r\n",
        "\r\n",
        "    current_shift_time = datetime.now() - timedelta(hours=5)\r\n",
        "    current_time = {'current_time': current_shift_time.strftime(\"%Y-%m-%d %H:%M:%S\")}\r\n",
        "    \r\n",
        "    current_time_df = pd.DataFrame(current_time, columns = ['current_time'], index=[0])\r\n",
        "\r\n",
        "    current_time_df['current_time'] =  pd.to_datetime(current_time_df['current_time'], format='%Y-%m-%d %H:%M:%S')\r\n",
        "\r\n",
        "    load_to_bigquery_current_time(current_time_df, time_schema)\r\n",
        "\r\n",
        "    # df.to_csv(\"datetime.csv\", index = False)\r\n",
        "\r\n",
        "   # return df\r\n",
        "\r\n",
        "def load_to_bigquery(df, listShift_schema):\r\n",
        "\r\n",
        "    df.to_csv(\"test1.csv\", index = False)\r\n",
        "    print(\"Inside Bigquery \")\r\n",
        "    table_id = \"hnmc-data-science.wheniwork.listShifts\"\r\n",
        "\r\n",
        "    table = client.get_table(table_id)  # Make an API request.\r\n",
        "    print(\r\n",
        "        \"before Loading new data {} rows and {} columns are present in {}\".format(\r\n",
        "            table.num_rows, len(table.schema), table_id\r\n",
        "        )\r\n",
        "    )\r\n",
        "    # Specifying all table schema\r\n",
        "    # with open('/listShift.json') as json_schema: \r\n",
        "    #     file_schema = json.load(json_schema)\r\n",
        "        \r\n",
        "    print(\"Inside schema\")\r\n",
        "    #print(file_schema['listShifts'])\r\n",
        "    job_config = bigquery.LoadJobConfig(\r\n",
        "        \r\n",
        "      #schema = listShift_schema,  \r\n",
        "      write_disposition=\"WRITE_APPEND\"\r\n",
        "    # Optionally, set the write disposition. BigQuery appends loaded rows\r\n",
        "    # to an existing table by default, but with WRITE_TRUNCATE write\r\n",
        "    # disposition it replaces the table with the loaded data.\r\n",
        "    )\r\n",
        "\r\n",
        "    job = client.load_table_from_dataframe(\r\n",
        "        df, table_id, job_config=job_config\r\n",
        "    )  # Make an API request.\r\n",
        "    job.result()  # Wait for the job to complete.\r\n",
        "\r\n",
        "    table = client.get_table(table_id)  # Make an API request.\r\n",
        "    print(\r\n",
        "        \"After Loading new data {} rows and {} columns are present in {}\".format(\r\n",
        "            table.num_rows, len(table.schema), table_id\r\n",
        "        )\r\n",
        "    )\r\n",
        "\r\n",
        "def load_to_bigquery_current_time(df, time_schema):\r\n",
        "\r\n",
        "    print(\"Inside Bigquery \")\r\n",
        "    project_id = 'hnmc-data-science'\r\n",
        "    dataset_id = 'wheniwork'\r\n",
        "\r\n",
        "    table_id = f\"{project_id}.{dataset_id}.currentShiftTime\"\r\n",
        "\r\n",
        "    # Specifying all table schema\r\n",
        "    # with open('/listShift.json') as json_schema: \r\n",
        "    #     file_schema = json.load(json_schema)\r\n",
        "    # print(type(file_schema[filename]))  \r\n",
        "    print(\"Inside current time schema\")\r\n",
        "   \r\n",
        "    #print(file_schema['listShifts'])\r\n",
        "    job_config = bigquery.LoadJobConfig(\r\n",
        "          \r\n",
        "      write_disposition=\"WRITE_TRUNCATE\", schema = time_schema\r\n",
        "      #, encoding = 'ISO-8859-1'\r\n",
        "    # Optionally, set the write disposition. BigQuery appends loaded rows\r\n",
        "    # to an existing table by default, but with WRITE_TRUNCATE write\r\n",
        "    # disposition it replaces the table with the loaded data.\r\n",
        "    )\r\n",
        "\r\n",
        "    job = client.load_table_from_dataframe(\r\n",
        "        df, table_id, job_config=job_config\r\n",
        "    )  # Make an API request.\r\n",
        "    job.result()  # Wait for the job to complete.\r\n",
        "\r\n",
        "    table = client.get_table(table_id)  # Make an API request.\r\n",
        "    print(\r\n",
        "        \"Loaded {} rows and {} columns to {}\".format(\r\n",
        "            table.num_rows, len(table.schema), table_id\r\n",
        "        )\r\n",
        "    )\r\n",
        "  \r\n",
        "main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "datetime64[ns]\n",
            "0      2020-10-31 09:00:39\n",
            "1      2020-10-31 13:00:41\n",
            "2      2020-10-31 17:00:24\n",
            "3      2020-10-31 17:00:23\n",
            "4      2020-10-31 17:00:23\n",
            "               ...        \n",
            "3957   2020-12-03 13:34:17\n",
            "3958   2020-12-03 13:34:17\n",
            "3959   2020-12-03 13:34:17\n",
            "3960   2020-12-03 13:34:17\n",
            "3961   2020-12-03 13:34:17\n",
            "Name: updated_at, Length: 3962, dtype: datetime64[ns]\n",
            "2020-12-11 15:09:19\n",
            "2020-12-11 15:09:19\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 0 entries\n",
            "Data columns (total 31 columns):\n",
            " #   Column                         Non-Null Count  Dtype         \n",
            "---  ------                         --------------  -----         \n",
            " 0   id                             0 non-null      int64         \n",
            " 1   account_id                     0 non-null      int64         \n",
            " 2   user_id                        0 non-null      int64         \n",
            " 3   location_id                    0 non-null      int64         \n",
            " 4   position_id                    0 non-null      int64         \n",
            " 5   site_id                        0 non-null      int64         \n",
            " 6   start_time                     0 non-null      datetime64[ns]\n",
            " 7   end_time                       0 non-null      datetime64[ns]\n",
            " 8   break_time                     0 non-null      float64       \n",
            " 9   color                          0 non-null      object        \n",
            " 10  notes                          0 non-null      object        \n",
            " 11  alerted                        0 non-null      bool          \n",
            " 12  linked_users                   0 non-null      object        \n",
            " 13  shiftchain_key                 0 non-null      object        \n",
            " 14  published                      0 non-null      bool          \n",
            " 15  published_date                 0 non-null      datetime64[ns]\n",
            " 16  notified_at                    0 non-null      object        \n",
            " 17  instances                      0 non-null      int64         \n",
            " 18  created_at                     0 non-null      datetime64[ns]\n",
            " 19  updated_at                     0 non-null      datetime64[ns]\n",
            " 20  acknowledged                   0 non-null      int64         \n",
            " 21  acknowledged_at                0 non-null      object        \n",
            " 22  creator_id                     0 non-null      int64         \n",
            " 23  is_open                        0 non-null      bool          \n",
            " 24  actionable                     0 non-null      bool          \n",
            " 25  block_id                       0 non-null      int64         \n",
            " 26  requires_openshift_approval    0 non-null      bool          \n",
            " 27  openshift_approval_request_id  0 non-null      int64         \n",
            " 28  is_shared                      0 non-null      int64         \n",
            " 29  UNIQUE_KEY                     0 non-null      object        \n",
            " 30  VERSION_DATE                   0 non-null      datetime64[ns]\n",
            "dtypes: bool(5), datetime64[ns](6), float64(1), int64(12), object(7)\n",
            "memory usage: 0.0+ bytes\n",
            "None\n",
            "Inside Bigquery \n",
            "before Loading new data 3962 rows and 31 columns are present in hnmc-data-science.wheniwork.listShifts\n",
            "Inside schema\n",
            "After Loading new data 3962 rows and 31 columns are present in hnmc-data-science.wheniwork.listShifts\n",
            "Inside Bigquery \n",
            "Inside current time schema\n",
            "Loaded 1 rows and 1 columns to hnmc-data-science.wheniwork.currentShiftTime\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVXwFoUUI9iV",
        "outputId": "173712d1-97fe-4c76-b88a-2c75fe382048"
      },
      "source": [
        "# Pushing 1st range data from 1st Nov 2020 to 31st jan 2020 final code  ---- third version of code while working with Updated_at >  current time\r\n",
        "\r\n",
        "import requests\r\n",
        "import json\r\n",
        "from datetime import datetime\r\n",
        "from datetime import timedelta\r\n",
        "import os\r\n",
        "import hashlib\r\n",
        "import datetime as dt\r\n",
        "import pandas as pd\r\n",
        "from copy import deepcopy\r\n",
        "from google.cloud import bigquery\r\n",
        "from google.oauth2 import service_account\r\n",
        "from google.cloud.exceptions import NotFound\r\n",
        "from pandas import DataFrame\r\n",
        "\r\n",
        "project_id = 'hnmc-data-science'\r\n",
        "dataset_id = 'wheniwork'\r\n",
        "table_id = 'listShifts'\r\n",
        "\r\n",
        "# TODO(developer): Set key_path to the path to the service account key\r\n",
        "#                  file.\r\n",
        "\r\n",
        "key_path = \"/hnmc-data-science-1e5996a1f37c.json\"\r\n",
        "\r\n",
        "credentials = service_account.Credentials.from_service_account_file(\r\n",
        "    key_path, scopes=[\"https://www.googleapis.com/auth/cloud-platform\"],\r\n",
        ")\r\n",
        "client = bigquery.Client(credentials=credentials, project=project_id)\r\n",
        "dataset = client.dataset(dataset_id)\r\n",
        "table_ref = dataset.table(table_id)\r\n",
        "\r\n",
        "time_schema = [\r\n",
        "    bigquery.SchemaField(\"current_time\", \"TIMESTAMP\", mode=\"REQUIRED\")]\r\n",
        "\r\n",
        "listShift_schema = [\r\n",
        "       bigquery.SchemaField(\"id\", \"INT64\", mode=\"REQUIRED\"),\r\n",
        "       bigquery.SchemaField(\"account_id\", \"INT64\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"user_id\", \"INT64\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"location_id\", \"INT64\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"position_id\", \"INT64\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"site_id\", \"INT64\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"start_time\", \"TIMESTAMP\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"end_time\", \"TIMESTAMP\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"break_time\", \"FLOAT64\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"color\", \"STRING\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"notes\", \"STRING\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"alerted\", \"BOOL\", mode=\"NULLABLE\"),\r\n",
        "      # bigquery.SchemaField(\"linked_users\", \"INT64\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"shiftchain_key\", \"STRING\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"published\", \"BOOL\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"published_date\", \"TIMESTAMP\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"notified_at\", \"TIMESTAMP\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"instances\", \"INT64\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"created_at\", \"TIMESTAMP\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"updated_at\", \"TIMESTAMP\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"acknowledged\", \"INT64\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"acknowledged_at\", \"TIMESTAMP\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"creator_id\", \"INT64\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"is_open\", \"BOOL\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"actionable\", \"BOOL\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"block_id\", \"INT64\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"requires_openshift_approval\", \"BOOL\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"openshift_approval_request_id\", \"INT64\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"is_shared\", \"INT64\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"UNIQUE_KEY\", \"STRING\", mode=\"REQUIRED\"),\r\n",
        "       bigquery.SchemaField(\"VERSION_DATE\", \"TIMESTAMP\", mode=\"NULLABLE\")\r\n",
        "       ]\r\n",
        "\r\n",
        "def main():\r\n",
        "\r\n",
        "  #  df = get_all_data()\r\n",
        "  #  load_to_bigquery(df)\r\n",
        "\r\n",
        "   last_query_time_df = fetch_data()\r\n",
        "   get_all_data(last_query_time_df)\r\n",
        "\r\n",
        "def fetch_data():\r\n",
        "\r\n",
        "    sql_source = \"\"\" SELECT * FROM `hnmc-data-science.wheniwork.currentShiftTime` \"\"\"             \r\n",
        "\r\n",
        "    df_source = client.query(sql_source).to_dataframe()\r\n",
        "    df_source['current_time'] = df_source['current_time'].astype(str).str[:-6]\r\n",
        "    df_source['current_time'] = pd.to_datetime(df_source['current_time'], format='%d %b %Y %H:%M:%S', errors='ignore')\r\n",
        "\r\n",
        "    return df_source\r\n",
        "\r\n",
        "def get_all_data(last_query_time_df):\r\n",
        "\r\n",
        "    r = requests.post('https://api.login.wheniwork.com/login', json={\"email\":\"rdoshi@holyname.org\",\"password\":\"wheniwork123\"})\r\n",
        "    r.status_code\r\n",
        "    data= r.json()\r\n",
        "\r\n",
        "    token=data[\"token\"]\r\n",
        "    #print(token)\r\n",
        "\r\n",
        "    f = open('/wiw_listShifts.json',) \r\n",
        "    data = json.load(f)\r\n",
        "\r\n",
        "    for key, value in data.items():\r\n",
        "\r\n",
        "      endpoint = f\"https://api.wheniwork.com/2/{data[key]['api_endpoint_name']}\"\r\n",
        "      print(endpoint)\r\n",
        "      headers = {\"Authorization\": token}\r\n",
        "\r\n",
        "      all_data = requests.get(endpoint, params={'start': '01 Nov 2020', 'end': '31 Jan 2021', 'include_allopen': True}, headers=headers).json()\r\n",
        "      print(all_data)\r\n",
        "      new_list = all_data[data[key]['api_endpoint_name']]\r\n",
        "      #print(new_list)\r\n",
        "\r\n",
        "      final_dataframe = []\r\n",
        "      for i in range(len(new_list)):\r\n",
        "\r\n",
        "        dictionary = {x: new_list[i][x] for x in data[key]['columns']}\r\n",
        "        final_dataframe.append(dictionary)\r\n",
        "\r\n",
        "      #print(final_dataframe)\r\n",
        "\r\n",
        "      # convert list of dictionary to dataframe\r\n",
        "      df = pd.DataFrame(final_dataframe)\r\n",
        "\r\n",
        "#     id_ofall_data = []\r\n",
        "#     for i in range(0, len(all_data['shifts'])):\r\n",
        "\r\n",
        "#       id_ofall_data.append(all_data['shifts'][i]['id'])\r\n",
        "\r\n",
        "#     print(id_ofall_data)\r\n",
        "\r\n",
        "#     new_list = []\r\n",
        "#     for i in range(0, len(all_data['shifts'])):\r\n",
        "\r\n",
        "#       if all_data['shifts'][i]['id'] in id_ofall_data: \r\n",
        "        \r\n",
        "#         #print(all_data['shifts'][i])\r\n",
        "#         new_list.append(all_data['shifts'][i])\r\n",
        "\r\n",
        "#     # convert list of dictionary to dataframe\r\n",
        "\r\n",
        "#     #print(new_list)\r\n",
        "#     df = pd.DataFrame(new_list)\r\n",
        "\r\n",
        "    df.to_csv(\"before_time_conv.csv\", index = False)\r\n",
        "    df['start_time'] = df['start_time'].astype(str).str[5:-6]\r\n",
        "    df['end_time'] = df['end_time'].astype(str).str[5:-6]\r\n",
        "    df['published_date'] = df['published_date'].astype(str).str[5:-6]\r\n",
        "    df['notified_at'] = df['notified_at'].astype(str).str[5:-6]\r\n",
        "    df['created_at'] = df['created_at'].astype(str).str[5:-6]\r\n",
        "    df['updated_at'] = df['updated_at'].astype(str).str[5:-6]\r\n",
        "    df['acknowledged_at'] = df['acknowledged_at'].astype(str).str[5:-6]\r\n",
        "    df.to_csv(\"after_time_conv.csv\", index = False)\r\n",
        "    # print(df)\r\n",
        "    df['start_time'] =  pd.to_datetime(df['start_time'], format='%d %b %Y %H:%M:%S', errors='ignore')\r\n",
        "    df['end_time'] =  pd.to_datetime(df['end_time'], format='%d %b %Y %H:%M:%S', errors='ignore')\r\n",
        "    df['published_date'] =  pd.to_datetime(df['published_date'], format='%d %b %Y %H:%M:%S', errors='ignore')\r\n",
        "    df['notified_at'] =  pd.to_datetime(df['notified_at'], format='%d %b %Y %H:%M:%S', errors='ignore')\r\n",
        "    df['created_at'] =  pd.to_datetime(df['created_at'], format='%d %b %Y %H:%M:%S', errors='ignore')\r\n",
        "    df['updated_at'] =  pd.to_datetime(df['updated_at'], format='%d %b %Y %H:%M:%S', errors='ignore')\r\n",
        "    df['acknowledged_at'] =  pd.to_datetime(df['acknowledged_at'], format='%d %b %Y %H:%M:%S', errors='ignore')\r\n",
        "   \r\n",
        "    print(df['updated_at'].dtypes)\r\n",
        "    #generating Hash key\r\n",
        "    gene_hash = {\r\n",
        "        'listShifts': ['id']\r\n",
        "    }\r\n",
        "\r\n",
        "    key_combination = gene_hash.get('listShifts')\r\n",
        "    \r\n",
        "    df['UNIQUE_KEY'] = df[key_combination].apply(lambda row: '_'.join(row.values.astype(\r\n",
        "        str)), axis=1).astype(str).str.encode('utf-8').apply(lambda x: (hashlib.sha512(x).hexdigest().upper()))\r\n",
        "\r\n",
        "    df['VERSION_DATE'] = datetime.now()\r\n",
        "\r\n",
        "    # check for updated data from API call\r\n",
        "\r\n",
        "    print(df['updated_at'])\r\n",
        "    print(last_query_time_df['current_time'][0])\r\n",
        "    last_query_time_df['current_time'] = last_query_time_df['current_time'].astype(str).str[:-6]\r\n",
        "    last_query_time_df['current_time'] =  pd.to_datetime(last_query_time_df['current_time'], format='%d %b %Y %H:%M:%S', errors='ignore')\r\n",
        "    print(last_query_time_df['current_time'][0])\r\n",
        "    new_data_df = df[df['updated_at'] >= (last_query_time_df['current_time'][0])]\r\n",
        "    print(new_data_df.info())\r\n",
        "    load_to_bigquery(new_data_df, listShift_schema)\r\n",
        "\r\n",
        "    # Assign current time to our new currentShiftTime table\r\n",
        "\r\n",
        "    current_shift_time = datetime.now() - timedelta(hours=5)\r\n",
        "    current_time = {'current_time': current_shift_time.strftime(\"%Y-%m-%d %H:%M:%S\")}\r\n",
        "    \r\n",
        "    current_time_df = pd.DataFrame(current_time, columns = ['current_time'], index=[0])\r\n",
        "\r\n",
        "    current_time_df['current_time'] =  pd.to_datetime(current_time_df['current_time'], format='%Y-%m-%d %H:%M:%S')\r\n",
        "\r\n",
        "    load_to_bigquery_current_time(current_time_df, time_schema)\r\n",
        "\r\n",
        "#     # df.to_csv(\"datetime.csv\", index = False)\r\n",
        "\r\n",
        "#    # return df\r\n",
        "\r\n",
        "def load_to_bigquery(df, listShift_schema):\r\n",
        "\r\n",
        "    df.to_csv(\"test1.csv\", index = False)\r\n",
        "    print(\"Inside Bigquery \")\r\n",
        "    table_id = \"hnmc-data-science.wheniwork.listShifts\"\r\n",
        "\r\n",
        "    table = client.get_table(table_id)  # Make an API request.\r\n",
        "    print(\r\n",
        "        \"before Loading new data {} rows and {} columns are present in {}\".format(\r\n",
        "            table.num_rows, len(table.schema), table_id\r\n",
        "        )\r\n",
        "    )\r\n",
        "    # Specifying all table schema\r\n",
        "    # with open('/listShift.json') as json_schema: \r\n",
        "    #     file_schema = json.load(json_schema)\r\n",
        "        \r\n",
        "    print(\"Inside schema\")\r\n",
        "    #print(file_schema['listShifts'])\r\n",
        "    job_config = bigquery.LoadJobConfig(\r\n",
        "        \r\n",
        "      #schema = listShift_schema,  \r\n",
        "      write_disposition=\"WRITE_APPEND\"\r\n",
        "    # Optionally, set the write disposition. BigQuery appends loaded rows\r\n",
        "    # to an existing table by default, but with WRITE_TRUNCATE write\r\n",
        "    # disposition it replaces the table with the loaded data.\r\n",
        "    )\r\n",
        "\r\n",
        "    job = client.load_table_from_dataframe(\r\n",
        "        df, table_id, job_config=job_config\r\n",
        "    )  # Make an API request.\r\n",
        "    job.result()  # Wait for the job to complete.\r\n",
        "\r\n",
        "    table = client.get_table(table_id)  # Make an API request.\r\n",
        "    print(\r\n",
        "        \"After Loading new data {} rows and {} columns are present in {}\".format(\r\n",
        "            table.num_rows, len(table.schema), table_id\r\n",
        "        )\r\n",
        "    )\r\n",
        "\r\n",
        "def load_to_bigquery_current_time(df, time_schema):\r\n",
        "\r\n",
        "    print(\"Inside Bigquery \")\r\n",
        "    project_id = 'hnmc-data-science'\r\n",
        "    dataset_id = 'wheniwork'\r\n",
        "\r\n",
        "    table_id = f\"{project_id}.{dataset_id}.currentShiftTime\"\r\n",
        "\r\n",
        "    # Specifying all table schema\r\n",
        "    # with open('/listShift.json') as json_schema: \r\n",
        "    #     file_schema = json.load(json_schema)\r\n",
        "    # print(type(file_schema[filename]))  \r\n",
        "    print(\"Inside current time schema\")\r\n",
        "   \r\n",
        "    #print(file_schema['listShifts'])\r\n",
        "    job_config = bigquery.LoadJobConfig(\r\n",
        "          \r\n",
        "      write_disposition=\"WRITE_TRUNCATE\", schema = time_schema\r\n",
        "      #, encoding = 'ISO-8859-1'\r\n",
        "    # Optionally, set the write disposition. BigQuery appends loaded rows\r\n",
        "    # to an existing table by default, but with WRITE_TRUNCATE write\r\n",
        "    # disposition it replaces the table with the loaded data.\r\n",
        "    )\r\n",
        "\r\n",
        "    job = client.load_table_from_dataframe(\r\n",
        "        df, table_id, job_config=job_config\r\n",
        "    )  # Make an API request.\r\n",
        "    job.result()  # Wait for the job to complete.\r\n",
        "\r\n",
        "    table = client.get_table(table_id)  # Make an API request.\r\n",
        "    print(\r\n",
        "        \"Loaded {} rows and {} columns to {}\".format(\r\n",
        "            table.num_rows, len(table.schema), table_id\r\n",
        "        )\r\n",
        "    )\r\n",
        "  \r\n",
        "main()"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://api.wheniwork.com/2/shifts\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "datetime64[ns]\n",
            "0      2020-10-31 09:00:39\n",
            "1      2020-10-31 13:00:41\n",
            "2      2020-10-31 17:00:24\n",
            "3      2020-10-31 17:00:23\n",
            "4      2020-10-31 17:00:23\n",
            "               ...        \n",
            "4629   2020-12-03 13:34:17\n",
            "4630   2020-12-03 13:34:17\n",
            "4631   2020-12-03 13:34:17\n",
            "4632   2020-12-03 13:34:17\n",
            "4633   2020-12-03 13:34:17\n",
            "Name: updated_at, Length: 4634, dtype: datetime64[ns]\n",
            "2020-12-11 16:03:34\n",
            "2020-12-11 16\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 695 entries, 2420 to 4578\n",
            "Data columns (total 30 columns):\n",
            " #   Column                         Non-Null Count  Dtype         \n",
            "---  ------                         --------------  -----         \n",
            " 0   id                             695 non-null    int64         \n",
            " 1   account_id                     695 non-null    int64         \n",
            " 2   user_id                        695 non-null    int64         \n",
            " 3   location_id                    695 non-null    int64         \n",
            " 4   position_id                    695 non-null    int64         \n",
            " 5   site_id                        695 non-null    int64         \n",
            " 6   start_time                     695 non-null    datetime64[ns]\n",
            " 7   end_time                       695 non-null    datetime64[ns]\n",
            " 8   break_time                     695 non-null    float64       \n",
            " 9   color                          695 non-null    object        \n",
            " 10  notes                          695 non-null    object        \n",
            " 11  alerted                        695 non-null    bool          \n",
            " 12  shiftchain_key                 695 non-null    object        \n",
            " 13  published                      695 non-null    bool          \n",
            " 14  published_date                 695 non-null    datetime64[ns]\n",
            " 15  notified_at                    695 non-null    object        \n",
            " 16  instances                      695 non-null    int64         \n",
            " 17  created_at                     695 non-null    datetime64[ns]\n",
            " 18  updated_at                     695 non-null    datetime64[ns]\n",
            " 19  acknowledged                   695 non-null    int64         \n",
            " 20  acknowledged_at                695 non-null    object        \n",
            " 21  creator_id                     695 non-null    int64         \n",
            " 22  is_open                        695 non-null    bool          \n",
            " 23  actionable                     695 non-null    bool          \n",
            " 24  block_id                       695 non-null    int64         \n",
            " 25  requires_openshift_approval    695 non-null    bool          \n",
            " 26  openshift_approval_request_id  695 non-null    int64         \n",
            " 27  is_shared                      695 non-null    int64         \n",
            " 28  UNIQUE_KEY                     695 non-null    object        \n",
            " 29  VERSION_DATE                   695 non-null    datetime64[ns]\n",
            "dtypes: bool(5), datetime64[ns](6), float64(1), int64(12), object(6)\n",
            "memory usage: 144.6+ KB\n",
            "None\n",
            "Inside Bigquery \n",
            "before Loading new data 3962 rows and 31 columns are present in hnmc-data-science.wheniwork.listShifts\n",
            "Inside schema\n",
            "After Loading new data 4657 rows and 31 columns are present in hnmc-data-science.wheniwork.listShifts\n",
            "Inside Bigquery \n",
            "Inside current time schema\n",
            "Loaded 1 rows and 1 columns to hnmc-data-science.wheniwork.currentShiftTime\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uakf5v4aGBCY"
      },
      "source": [
        "# Pushing 1st range data from 1st Nov 2020 to 31st jan 2020 ==== final code 11th Dec 2020 10 PM \r\n",
        "\r\n",
        "import requests\r\n",
        "import json\r\n",
        "from datetime import datetime\r\n",
        "from datetime import timedelta\r\n",
        "import os\r\n",
        "import hashlib\r\n",
        "import datetime as dt\r\n",
        "import pandas as pd\r\n",
        "from copy import deepcopy\r\n",
        "from google.cloud import bigquery\r\n",
        "from google.oauth2 import service_account\r\n",
        "from google.cloud.exceptions import NotFound\r\n",
        "from pandas import DataFrame\r\n",
        "\r\n",
        "project_id = 'hnmc-data-science'\r\n",
        "dataset_id = 'wheniwork'\r\n",
        "table_id = 'listShifts'\r\n",
        "\r\n",
        "# TODO(developer): Set key_path to the path to the service account key\r\n",
        "#                  file.\r\n",
        "\r\n",
        "key_path = \"/hnmc-data-science-1e5996a1f37c.json\"\r\n",
        "\r\n",
        "credentials = service_account.Credentials.from_service_account_file(\r\n",
        "    key_path, scopes=[\"https://www.googleapis.com/auth/cloud-platform\"],\r\n",
        ")\r\n",
        "client = bigquery.Client(credentials=credentials, project=project_id)\r\n",
        "dataset = client.dataset(dataset_id)\r\n",
        "table_ref = dataset.table(table_id)\r\n",
        "time_schema = [\r\n",
        "    bigquery.SchemaField(\"current_time\", \"TIMESTAMP\", mode=\"REQUIRED\")]\r\n",
        "\r\n",
        "def main():\r\n",
        "\r\n",
        "    df = get_all_data()\r\n",
        "    load_to_bigquery(df)\r\n",
        "\r\n",
        "def get_all_data():\r\n",
        "\r\n",
        "    r = requests.post('https://api.login.wheniwork.com/login', json={\"email\":\"rdoshi@holyname.org\",\"password\":\"wheniwork123\"})\r\n",
        "    r.status_code\r\n",
        "    data= r.json()\r\n",
        "\r\n",
        "    token=data[\"token\"]\r\n",
        "    #print(token)\r\n",
        "    endpoint = \"https://api.wheniwork.com/2/shifts\"\r\n",
        "\r\n",
        "    headers = {\"Authorization\": token}\r\n",
        "\r\n",
        "    all_data = requests.get(endpoint, params={'start': '01 Nov 2020', 'end': '31 Jan 2021', 'include_allopen': True}, headers=headers).json()\r\n",
        "    print(all_data)\r\n",
        "\r\n",
        "    id_ofall_data = []\r\n",
        "    for i in range(0, len(all_data['shifts'])):\r\n",
        "\r\n",
        "      id_ofall_data.append(all_data['shifts'][i]['id'])\r\n",
        "\r\n",
        "    print(id_ofall_data)\r\n",
        "\r\n",
        "    new_list = []\r\n",
        "    for i in range(0, len(all_data['shifts'])):\r\n",
        "\r\n",
        "      if all_data['shifts'][i]['id'] in id_ofall_data: \r\n",
        "        \r\n",
        "        #print(all_data['shifts'][i])\r\n",
        "        new_list.append(all_data['shifts'][i])\r\n",
        "\r\n",
        "    # convert list of dictionary to dataframe\r\n",
        "\r\n",
        "    #print(new_list)\r\n",
        "    df = pd.DataFrame(new_list)\r\n",
        "    # print(df)\r\n",
        "    df['start_time'] =  pd.to_datetime(df['start_time'], format='%a, %d %b %Y %H:%M:%S %z', errors='ignore')\r\n",
        "    df['end_time'] =  pd.to_datetime(df['end_time'], format='%a, %d %b %Y %H:%M:%S %z', errors='ignore')\r\n",
        "    df['published_date'] =  pd.to_datetime(df['published_date'], format='%a, %d %b %Y %H:%M:%S %z', errors='ignore')\r\n",
        "    df['notified_at'] =  pd.to_datetime(df['notified_at'], format='%a, %d %b %Y %H:%M:%S %z', errors='ignore')\r\n",
        "    df['created_at'] =  pd.to_datetime(df['created_at'], format='%a, %d %b %Y %H:%M:%S %z', errors='ignore')\r\n",
        "    df['updated_at'] =  pd.to_datetime(df['updated_at'], format='%a, %d %b %Y %H:%M:%S %z', errors='ignore')\r\n",
        "    df['acknowledged_at'] =  pd.to_datetime(df['acknowledged_at'], format='%a, %d %b %Y %H:%M:%S %z', errors='ignore')\r\n",
        "    print(df['updated_at'].dtypes)\r\n",
        "    print(df['updated_at'])\r\n",
        "    df['updated_at'] = df['updated_at'].astype(str).str[:-6]\r\n",
        "    df['updated_at'] = pd.to_datetime(df['updated_at'])\r\n",
        "    print(df['updated_at'].dtypes)\r\n",
        "    print(df['updated_at'])\r\n",
        "    #generating Hash key\r\n",
        "    gene_hash = {\r\n",
        "        'listShifts': ['id']\r\n",
        "    }\r\n",
        "\r\n",
        "    key_combination = gene_hash.get('listShifts')\r\n",
        "    \r\n",
        "    df['UNIQUE_KEY'] = df[key_combination].apply(lambda row: '_'.join(row.values.astype(\r\n",
        "        str)), axis=1).astype(str).str.encode('utf-8').apply(lambda x: (hashlib.sha512(x).hexdigest().upper()))\r\n",
        "\r\n",
        "    df['VERSION_DATE'] = datetime.now()\r\n",
        "    df.drop('linked_users', axis=1, inplace=True)\r\n",
        "\r\n",
        "    df.to_csv(\"test.csv\", index = False)\r\n",
        "\r\n",
        "    current_shift_time = datetime.now() - timedelta(hours=5)\r\n",
        "    current_time = {'current_time': current_shift_time.strftime(\"%Y-%m-%d %H:%M:%S\")}\r\n",
        "    \r\n",
        "    current_time_df = pd.DataFrame(current_time, columns = ['current_time'], index=[0])\r\n",
        "\r\n",
        "    current_time_df['current_time'] =  pd.to_datetime(current_time_df['current_time'], format='%Y-%m-%d %H:%M:%S')\r\n",
        "\r\n",
        "    load_to_bigquery_current_time(current_time_df, time_schema)\r\n",
        "\r\n",
        "    return df\r\n",
        "\r\n",
        "def load_to_bigquery(df):\r\n",
        "\r\n",
        "    print(\"Inside Bigquery \")\r\n",
        "    table_id = \"hnmc-data-science.wheniwork.listShifts\"\r\n",
        "\r\n",
        "    # Specifying all table schema\r\n",
        "    # with open('/listShift.json') as json_schema: \r\n",
        "    #     file_schema = json.load(json_schema)\r\n",
        "        \r\n",
        "    print(\"Inside schema\")\r\n",
        "    #print(file_schema['listShifts'])\r\n",
        "    job_config = bigquery.LoadJobConfig(\r\n",
        "        \r\n",
        "      schema = [\r\n",
        "       bigquery.SchemaField(\"id\", \"INT64\", mode=\"REQUIRED\"),\r\n",
        "       bigquery.SchemaField(\"account_id\", \"INT64\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"user_id\", \"INT64\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"location_id\", \"INT64\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"position_id\", \"INT64\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"site_id\", \"INT64\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"start_time\", \"TIMESTAMP\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"end_time\", \"TIMESTAMP\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"break_time\", \"FLOAT64\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"color\", \"STRING\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"notes\", \"STRING\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"alerted\", \"BOOL\", mode=\"NULLABLE\"),\r\n",
        "       #bigquery.SchemaField(\"linked_users\", \"INT64\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"shiftchain_key\", \"STRING\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"published\", \"BOOL\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"published_date\", \"TIMESTAMP\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"notified_at\", \"TIMESTAMP\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"instances\", \"INT64\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"created_at\", \"TIMESTAMP\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"updated_at\", \"TIMESTAMP\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"acknowledged\", \"INT64\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"acknowledged_at\", \"TIMESTAMP\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"creator_id\", \"INT64\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"is_open\", \"BOOL\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"actionable\", \"BOOL\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"block_id\", \"INT64\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"requires_openshift_approval\", \"BOOL\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"openshift_approval_request_id\", \"INT64\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"is_shared\", \"INT64\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"UNIQUE_KEY\", \"STRING\", mode=\"REQUIRED\"),\r\n",
        "       bigquery.SchemaField(\"VERSION_DATE\", \"TIMESTAMP\", mode=\"NULLABLE\")\r\n",
        "       ],  \r\n",
        "      write_disposition=\"WRITE_APPEND\"\r\n",
        "    # Optionally, set the write disposition. BigQuery appends loaded rows\r\n",
        "    # to an existing table by default, but with WRITE_TRUNCATE write\r\n",
        "    # disposition it replaces the table with the loaded data.\r\n",
        "    )\r\n",
        "\r\n",
        "    job = client.load_table_from_dataframe(\r\n",
        "        df, table_id, job_config=job_config\r\n",
        "    )  # Make an API request.\r\n",
        "    job.result()  # Wait for the job to complete.\r\n",
        "\r\n",
        "    table = client.get_table(table_id)  # Make an API request.\r\n",
        "    print(\r\n",
        "        \"Loaded {} rows and {} columns to {}\".format(\r\n",
        "            table.num_rows, len(table.schema), table_id\r\n",
        "        )\r\n",
        "    )\r\n",
        "\r\n",
        "def load_to_bigquery_current_time(df, time_schema):\r\n",
        "\r\n",
        "    print(\"Inside Bigquery \")\r\n",
        "    project_id = 'hnmc-data-science'\r\n",
        "    dataset_id = 'wheniwork'\r\n",
        "\r\n",
        "    table_id = f\"{project_id}.{dataset_id}.currentShiftTime\"\r\n",
        "\r\n",
        "    # Specifying all table schema\r\n",
        "    # with open('/listShift.json') as json_schema: \r\n",
        "    #     file_schema = json.load(json_schema)\r\n",
        "    # print(type(file_schema[filename]))  \r\n",
        "    print(\"Inside schema\")\r\n",
        "   \r\n",
        "    #print(file_schema['listShifts'])\r\n",
        "    job_config = bigquery.LoadJobConfig(\r\n",
        "          \r\n",
        "      write_disposition=\"WRITE_TRUNCATE\", schema = time_schema\r\n",
        "      #, encoding = 'ISO-8859-1'\r\n",
        "    # Optionally, set the write disposition. BigQuery appends loaded rows\r\n",
        "    # to an existing table by default, but with WRITE_TRUNCATE write\r\n",
        "    # disposition it replaces the table with the loaded data.\r\n",
        "    )\r\n",
        "\r\n",
        "    job = client.load_table_from_dataframe(\r\n",
        "        df, table_id, job_config=job_config\r\n",
        "    )  # Make an API request.\r\n",
        "    job.result()  # Wait for the job to complete.\r\n",
        "\r\n",
        "    table = client.get_table(table_id)  # Make an API request.\r\n",
        "    print(\r\n",
        "        \"Loaded {} rows and {} columns to {}\".format(\r\n",
        "            table.num_rows, len(table.schema), table_id\r\n",
        "        )\r\n",
        "    )\r\n",
        "        \r\n",
        "main()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MOSdv6U49hdo",
        "outputId": "dc4b010c-a5f4-4550-ce16-a3dfcc935fd6"
      },
      "source": [
        "# Pushing 1st range data from 1st Nov 2020 to 31st jan 2020 final code  ---- \r\n",
        "# second version of code while working with Updated_at >  current time 11th DEC 2020 10:20 PM\r\n",
        "\r\n",
        "import requests\r\n",
        "import json\r\n",
        "from datetime import datetime\r\n",
        "from datetime import timedelta\r\n",
        "import os\r\n",
        "import hashlib\r\n",
        "import datetime as dt\r\n",
        "import pandas as pd\r\n",
        "from copy import deepcopy\r\n",
        "from google.cloud import bigquery\r\n",
        "from google.oauth2 import service_account\r\n",
        "from google.cloud.exceptions import NotFound\r\n",
        "from pandas import DataFrame\r\n",
        "\r\n",
        "project_id = 'hnmc-data-science'\r\n",
        "dataset_id = 'wheniwork'\r\n",
        "table_id = 'listShifts'\r\n",
        "\r\n",
        "# TODO(developer): Set key_path to the path to the service account key\r\n",
        "#                  file.\r\n",
        "\r\n",
        "key_path = \"/hnmc-data-science-1e5996a1f37c.json\"\r\n",
        "\r\n",
        "credentials = service_account.Credentials.from_service_account_file(\r\n",
        "    key_path, scopes=[\"https://www.googleapis.com/auth/cloud-platform\"],\r\n",
        ")\r\n",
        "client = bigquery.Client(credentials=credentials, project=project_id)\r\n",
        "dataset = client.dataset(dataset_id)\r\n",
        "table_ref = dataset.table(table_id)\r\n",
        "\r\n",
        "time_schema = [\r\n",
        "    bigquery.SchemaField(\"current_time\", \"TIMESTAMP\", mode=\"REQUIRED\")]\r\n",
        "\r\n",
        "listShift_schema = [\r\n",
        "       bigquery.SchemaField(\"id\", \"INT64\", mode=\"REQUIRED\"),\r\n",
        "       bigquery.SchemaField(\"account_id\", \"INT64\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"user_id\", \"INT64\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"location_id\", \"INT64\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"position_id\", \"INT64\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"site_id\", \"INT64\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"start_time\", \"TIMESTAMP\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"end_time\", \"TIMESTAMP\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"break_time\", \"FLOAT64\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"color\", \"STRING\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"notes\", \"STRING\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"alerted\", \"BOOL\", mode=\"NULLABLE\"),\r\n",
        "       #bigquery.SchemaField(\"linked_users\", \"INT64\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"shiftchain_key\", \"STRING\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"published\", \"BOOL\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"published_date\", \"TIMESTAMP\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"notified_at\", \"TIMESTAMP\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"instances\", \"INT64\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"created_at\", \"TIMESTAMP\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"updated_at\", \"TIMESTAMP\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"acknowledged\", \"INT64\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"acknowledged_at\", \"TIMESTAMP\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"creator_id\", \"INT64\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"is_open\", \"BOOL\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"actionable\", \"BOOL\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"block_id\", \"INT64\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"requires_openshift_approval\", \"BOOL\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"openshift_approval_request_id\", \"INT64\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"is_shared\", \"INT64\", mode=\"NULLABLE\"),\r\n",
        "       bigquery.SchemaField(\"UNIQUE_KEY\", \"STRING\", mode=\"REQUIRED\"),\r\n",
        "       bigquery.SchemaField(\"VERSION_DATE\", \"TIMESTAMP\", mode=\"NULLABLE\")\r\n",
        "       ]\r\n",
        "\r\n",
        "def main():\r\n",
        "\r\n",
        "  #  df = get_all_data()\r\n",
        "  #  load_to_bigquery(df)\r\n",
        "\r\n",
        "   last_query_time_df = fetch_data()\r\n",
        "   get_all_data(last_query_time_df)\r\n",
        "\r\n",
        "def fetch_data():\r\n",
        "\r\n",
        "    sql_source = \"\"\" SELECT * FROM `hnmc-data-science.wheniwork.currentShiftTime` \"\"\"             \r\n",
        "\r\n",
        "    df_source = client.query(sql_source).to_dataframe()\r\n",
        "    df_source['current_time'] = df_source['current_time'].astype(str).str[:-6]\r\n",
        "    df_source['current_time'] = pd.to_datetime(df_source['current_time'], format='%d %b %Y %H:%M:%S', errors='ignore')\r\n",
        "\r\n",
        "    return df_source\r\n",
        "\r\n",
        "def get_all_data(last_query_time_df):\r\n",
        "\r\n",
        "    r = requests.post('https://api.login.wheniwork.com/login', json={\"email\":\"rdoshi@holyname.org\",\"password\":\"wheniwork123\"})\r\n",
        "    r.status_code\r\n",
        "    data= r.json()\r\n",
        "\r\n",
        "    token=data[\"token\"]\r\n",
        "    #print(token)\r\n",
        "    endpoint = \"https://api.wheniwork.com/2/shifts\"\r\n",
        "\r\n",
        "    headers = {\"Authorization\": token}\r\n",
        "\r\n",
        "    all_data = requests.get(endpoint, params={'start': '01 Nov 2020', 'end': '31 Jan 2021', 'include_allopen': True}, headers=headers).json()\r\n",
        "    print(all_data)\r\n",
        "\r\n",
        "    id_ofall_data = []\r\n",
        "    for i in range(0, len(all_data['shifts'])):\r\n",
        "\r\n",
        "      id_ofall_data.append(all_data['shifts'][i]['id'])\r\n",
        "\r\n",
        "    print(id_ofall_data)\r\n",
        "\r\n",
        "    new_list = []\r\n",
        "    for i in range(0, len(all_data['shifts'])):\r\n",
        "\r\n",
        "      if all_data['shifts'][i]['id'] in id_ofall_data: \r\n",
        "        \r\n",
        "        #print(all_data['shifts'][i])\r\n",
        "        new_list.append(all_data['shifts'][i])\r\n",
        "\r\n",
        "    # convert list of dictionary to dataframe\r\n",
        "\r\n",
        "    #print(new_list)\r\n",
        "    df = pd.DataFrame(new_list)\r\n",
        "\r\n",
        "    # df.to_csv(\"before_time_conv.csv\", index = False)\r\n",
        "    # df['start_time'] = df['start_time'].astype(str).str[5:-6]\r\n",
        "    # df['end_time'] = df['end_time'].astype(str).str[5:-6]\r\n",
        "    # df['published_date'] = df['published_date'].astype(str).str[5:-6]\r\n",
        "    # df['notified_at'] = df['notified_at'].astype(str).str[5:-6]\r\n",
        "    # df['created_at'] = df['created_at'].astype(str).str[5:-6]\r\n",
        "    # df['updated_at'] = df['updated_at'].astype(str).str[5:-6]\r\n",
        "    # df['acknowledged_at'] = df['acknowledged_at'].astype(str).str[5:-6]\r\n",
        "    # df.to_csv(\"after_time_conv.csv\", index = False)\r\n",
        "    # # print(df)\r\n",
        "    # df['start_time'] =  pd.to_datetime(df['start_time'], format='%d %b %Y %H:%M:%S', errors='ignore')\r\n",
        "    # df['end_time'] =  pd.to_datetime(df['end_time'], format='%d %b %Y %H:%M:%S', errors='ignore')\r\n",
        "    # df['published_date'] =  pd.to_datetime(df['published_date'], format='%d %b %Y %H:%M:%S', errors='ignore')\r\n",
        "    # df['notified_at'] =  pd.to_datetime(df['notified_at'], format='%d %b %Y %H:%M:%S', errors='ignore')\r\n",
        "    # df['created_at'] =  pd.to_datetime(df['created_at'], format='%d %b %Y %H:%M:%S', errors='ignore')\r\n",
        "    # df['updated_at'] =  pd.to_datetime(df['updated_at'], format='%d %b %Y %H:%M:%S', errors='ignore')\r\n",
        "    # df['acknowledged_at'] =  pd.to_datetime(df['acknowledged_at'], format='%d %b %Y %H:%M:%S', errors='ignore')\r\n",
        "\r\n",
        "    df['start_time'] =  pd.to_datetime(df['start_time'], format='%a, %d %b %Y %H:%M:%S %z', errors='ignore')\r\n",
        "    df['end_time'] =  pd.to_datetime(df['end_time'], format='%a, %d %b %Y %H:%M:%S %z', errors='ignore')\r\n",
        "    df['published_date'] =  pd.to_datetime(df['published_date'], format='%a, %d %b %Y %H:%M:%S %z', errors='ignore')\r\n",
        "    df['notified_at'] =  pd.to_datetime(df['notified_at'], format='%a, %d %b %Y %H:%M:%S %z', errors='ignore')\r\n",
        "    df['created_at'] =  pd.to_datetime(df['created_at'], format='%a, %d %b %Y %H:%M:%S %z', errors='ignore')\r\n",
        "    df['updated_at'] =  pd.to_datetime(df['updated_at'], format='%a, %d %b %Y %H:%M:%S %z', errors='ignore')\r\n",
        "    df['acknowledged_at'] =  pd.to_datetime(df['acknowledged_at'], format='%a, %d %b %Y %H:%M:%S %z', errors='ignore')\r\n",
        "    print(df['updated_at'].dtypes)\r\n",
        "    print(df['updated_at'])\r\n",
        "    df['updated_at'] = df['updated_at'].astype(str).str[:-6]\r\n",
        "    df['updated_at'] = pd.to_datetime(df['updated_at'])\r\n",
        "    print(df['updated_at'].dtypes)\r\n",
        "\r\n",
        "    #generating Hash key\r\n",
        "    gene_hash = {\r\n",
        "        'listShifts': ['id']\r\n",
        "    }\r\n",
        "\r\n",
        "    key_combination = gene_hash.get('listShifts')\r\n",
        "    \r\n",
        "    df['UNIQUE_KEY'] = df[key_combination].apply(lambda row: '_'.join(row.values.astype(\r\n",
        "        str)), axis=1).astype(str).str.encode('utf-8').apply(lambda x: (hashlib.sha512(x).hexdigest().upper()))\r\n",
        "\r\n",
        "    df['VERSION_DATE'] = datetime.now()\r\n",
        "    df.drop('linked_users', axis=1, inplace=True)\r\n",
        "\r\n",
        "\r\n",
        "    # check for updated data from API call\r\n",
        "\r\n",
        "    print(df['updated_at'])\r\n",
        "    print(last_query_time_df['current_time'][0])\r\n",
        "    # last_query_time_df['current_time'] = last_query_time_df['current_time'].astype(str).str[:-6]\r\n",
        "    # last_query_time_df['current_time'] =  pd.to_datetime(last_query_time_df['current_time'], format='%d %b %Y %H:%M:%S', errors='ignore')\r\n",
        "    print(last_query_time_df['current_time'][0])\r\n",
        "    new_data_df = df[df['updated_at'] >= (last_query_time_df['current_time'][0])]\r\n",
        "    print(new_data_df.info())\r\n",
        "    load_to_bigquery(new_data_df, listShift_schema)\r\n",
        "\r\n",
        "    # Assign current time to our new currentShiftTime table\r\n",
        "\r\n",
        "    current_shift_time = datetime.now() - timedelta(hours=5)\r\n",
        "    current_time = {'current_time': current_shift_time.strftime(\"%Y-%m-%d %H:%M:%S\")}\r\n",
        "    \r\n",
        "    current_time_df = pd.DataFrame(current_time, columns = ['current_time'], index=[0])\r\n",
        "\r\n",
        "    current_time_df['current_time'] =  pd.to_datetime(current_time_df['current_time'], format='%Y-%m-%d %H:%M:%S')\r\n",
        "\r\n",
        "    load_to_bigquery_current_time(current_time_df, time_schema)\r\n",
        "\r\n",
        "    # df.to_csv(\"datetime.csv\", index = False)\r\n",
        "\r\n",
        "   # return df\r\n",
        "\r\n",
        "def load_to_bigquery(df, listShift_schema):\r\n",
        "\r\n",
        "    df.to_csv(\"test1.csv\", index = False)\r\n",
        "    print(\"Inside Bigquery \")\r\n",
        "    table_id = \"hnmc-data-science.wheniwork.listShifts\"\r\n",
        "\r\n",
        "    table = client.get_table(table_id)  # Make an API request.\r\n",
        "    print(\r\n",
        "        \"before Loading new data {} rows and {} columns are present in {}\".format(\r\n",
        "            table.num_rows, len(table.schema), table_id\r\n",
        "        )\r\n",
        "    )\r\n",
        "    # Specifying all table schema\r\n",
        "    # with open('/listShift.json') as json_schema: \r\n",
        "    #     file_schema = json.load(json_schema)\r\n",
        "        \r\n",
        "    print(\"Inside schema\")\r\n",
        "    #print(file_schema['listShifts'])\r\n",
        "    job_config = bigquery.LoadJobConfig(\r\n",
        "        \r\n",
        "      #schema = listShift_schema,  \r\n",
        "      write_disposition=\"WRITE_APPEND\"\r\n",
        "    # Optionally, set the write disposition. BigQuery appends loaded rows\r\n",
        "    # to an existing table by default, but with WRITE_TRUNCATE write\r\n",
        "    # disposition it replaces the table with the loaded data.\r\n",
        "    )\r\n",
        "\r\n",
        "    job = client.load_table_from_dataframe(\r\n",
        "        df, table_id, job_config=job_config\r\n",
        "    )  # Make an API request.\r\n",
        "    job.result()  # Wait for the job to complete.\r\n",
        "\r\n",
        "    table = client.get_table(table_id)  # Make an API request.\r\n",
        "    print(\r\n",
        "        \"After Loading new data {} rows and {} columns are present in {}\".format(\r\n",
        "            table.num_rows, len(table.schema), table_id\r\n",
        "        )\r\n",
        "    )\r\n",
        "\r\n",
        "def load_to_bigquery_current_time(df, time_schema):\r\n",
        "\r\n",
        "    print(\"Inside Bigquery \")\r\n",
        "    project_id = 'hnmc-data-science'\r\n",
        "    dataset_id = 'wheniwork'\r\n",
        "\r\n",
        "    table_id = f\"{project_id}.{dataset_id}.currentShiftTime\"\r\n",
        "\r\n",
        "    # Specifying all table schema\r\n",
        "    # with open('/listShift.json') as json_schema: \r\n",
        "    #     file_schema = json.load(json_schema)\r\n",
        "    # print(type(file_schema[filename]))  \r\n",
        "    print(\"Inside current time schema\")\r\n",
        "   \r\n",
        "    #print(file_schema['listShifts'])\r\n",
        "    job_config = bigquery.LoadJobConfig(\r\n",
        "          \r\n",
        "      write_disposition=\"WRITE_TRUNCATE\", schema = time_schema\r\n",
        "      #, encoding = 'ISO-8859-1'\r\n",
        "    # Optionally, set the write disposition. BigQuery appends loaded rows\r\n",
        "    # to an existing table by default, but with WRITE_TRUNCATE write\r\n",
        "    # disposition it replaces the table with the loaded data.\r\n",
        "    )\r\n",
        "\r\n",
        "    job = client.load_table_from_dataframe(\r\n",
        "        df, table_id, job_config=job_config\r\n",
        "    )  # Make an API request.\r\n",
        "    job.result()  # Wait for the job to complete.\r\n",
        "\r\n",
        "    table = client.get_table(table_id)  # Make an API request.\r\n",
        "    print(\r\n",
        "        \"Loaded {} rows and {} columns to {}\".format(\r\n",
        "            table.num_rows, len(table.schema), table_id\r\n",
        "        )\r\n",
        "    )\r\n",
        "  \r\n",
        "main()"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "object\n",
            "0       2020-10-31 09:00:39-04:00\n",
            "1       2020-10-31 13:00:41-04:00\n",
            "2       2020-10-31 17:00:24-04:00\n",
            "3       2020-10-31 17:00:23-04:00\n",
            "4       2020-10-31 17:00:23-04:00\n",
            "                  ...            \n",
            "4629    2020-12-03 13:34:17-05:00\n",
            "4630    2020-12-03 13:34:17-05:00\n",
            "4631    2020-12-03 13:34:17-05:00\n",
            "4632    2020-12-03 13:34:17-05:00\n",
            "4633    2020-12-03 13:34:17-05:00\n",
            "Name: updated_at, Length: 4634, dtype: object\n",
            "datetime64[ns]\n",
            "0      2020-10-31 09:00:39\n",
            "1      2020-10-31 13:00:41\n",
            "2      2020-10-31 17:00:24\n",
            "3      2020-10-31 17:00:23\n",
            "4      2020-10-31 17:00:23\n",
            "               ...        \n",
            "4629   2020-12-03 13:34:17\n",
            "4630   2020-12-03 13:34:17\n",
            "4631   2020-12-03 13:34:17\n",
            "4632   2020-12-03 13:34:17\n",
            "4633   2020-12-03 13:34:17\n",
            "Name: updated_at, Length: 4634, dtype: datetime64[ns]\n",
            "2020-12-11 22:00:20\n",
            "2020-12-11 22:00:20\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 0 entries\n",
            "Data columns (total 30 columns):\n",
            " #   Column                         Non-Null Count  Dtype                                 \n",
            "---  ------                         --------------  -----                                 \n",
            " 0   id                             0 non-null      int64                                 \n",
            " 1   account_id                     0 non-null      int64                                 \n",
            " 2   user_id                        0 non-null      int64                                 \n",
            " 3   location_id                    0 non-null      int64                                 \n",
            " 4   position_id                    0 non-null      int64                                 \n",
            " 5   site_id                        0 non-null      int64                                 \n",
            " 6   start_time                     0 non-null      object                                \n",
            " 7   end_time                       0 non-null      datetime64[ns, pytz.FixedOffset(-300)]\n",
            " 8   break_time                     0 non-null      float64                               \n",
            " 9   color                          0 non-null      object                                \n",
            " 10  notes                          0 non-null      object                                \n",
            " 11  alerted                        0 non-null      bool                                  \n",
            " 12  shiftchain_key                 0 non-null      object                                \n",
            " 13  published                      0 non-null      bool                                  \n",
            " 14  published_date                 0 non-null      object                                \n",
            " 15  notified_at                    0 non-null      object                                \n",
            " 16  instances                      0 non-null      int64                                 \n",
            " 17  created_at                     0 non-null      object                                \n",
            " 18  updated_at                     0 non-null      datetime64[ns]                        \n",
            " 19  acknowledged                   0 non-null      int64                                 \n",
            " 20  acknowledged_at                0 non-null      object                                \n",
            " 21  creator_id                     0 non-null      int64                                 \n",
            " 22  is_open                        0 non-null      bool                                  \n",
            " 23  actionable                     0 non-null      bool                                  \n",
            " 24  block_id                       0 non-null      int64                                 \n",
            " 25  requires_openshift_approval    0 non-null      bool                                  \n",
            " 26  openshift_approval_request_id  0 non-null      int64                                 \n",
            " 27  is_shared                      0 non-null      int64                                 \n",
            " 28  UNIQUE_KEY                     0 non-null      object                                \n",
            " 29  VERSION_DATE                   0 non-null      datetime64[ns]                        \n",
            "dtypes: bool(5), datetime64[ns, pytz.FixedOffset(-300)](1), datetime64[ns](2), float64(1), int64(12), object(9)\n",
            "memory usage: 0.0+ bytes\n",
            "None\n",
            "Inside Bigquery \n",
            "before Loading new data 4634 rows and 30 columns are present in hnmc-data-science.wheniwork.listShifts\n",
            "Inside schema\n",
            "After Loading new data 4634 rows and 30 columns are present in hnmc-data-science.wheniwork.listShifts\n",
            "Inside Bigquery \n",
            "Inside current time schema\n",
            "Loaded 1 rows and 1 columns to hnmc-data-science.wheniwork.currentShiftTime\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dNQD6E9-wpA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}